{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Data Preprocessing <a name='data-preprocess'></a>\n",
    "[Back to top](#Contents)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/data_eng.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we one-hot encode the categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.get_dummies(data, prefix='type', columns=['Type'])\n",
    "data = data[[c for c in data if c not in ['price_sqft', 'type_Condo', 'type_HDB']] + ['type_HDB', 'type_Condo','price_sqft']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we stratified-split the dataset into training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdb = data[data['type_HDB'] == 1]\n",
    "condo = data[data['type_Condo'] == 1]\n",
    "\n",
    "train_hdb, test_hdb = train_test_split(hdb, test_size=0.1)\n",
    "train_condo, test_condo = train_test_split(condo, test_size=0.1)\n",
    "\n",
    "train = pd.concat([train_hdb, train_condo], axis=0).reset_index(drop=True)\n",
    "test = pd.concat([test_hdb, test_condo], axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that, we scaled the features of the training data by z-scoring. We are careful not to introduce any data leakage by using the same scaler to scale the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "train.iloc[:,:-3] = scaler.fit_transform(train.iloc[:,:-3]) #Excludes type_hdb, type_condo, price_sqft\n",
    "test.iloc[:,:-3] = scaler.transform(test.iloc[:,:-3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we store the training and test data into their respective files, and also the scaler used to scale the training data. This scaler can be used to scale any future test data we used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv('../data/train.csv', index=False)\n",
    "test.to_csv('../data/test.csv', index=False)\n",
    "pickle.dump(scaler, open('../model/object/scaler.pickle', 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
